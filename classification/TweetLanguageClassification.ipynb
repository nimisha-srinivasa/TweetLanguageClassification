{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, os, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, itertools\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "'''\n",
    "all constants\n",
    "'''\n",
    "\n",
    "data_dir_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'all_data'))\n",
    "input_fname = \"processed_data.json\"\n",
    "category_fname = \"uniformly_sampled.tsv\"\n",
    "\n",
    "#category labels according to the website\n",
    "'''\n",
    "category_labels = ['am', 'ar', 'bg', 'bn', 'bo', 'bs', 'ca', 'ckb', \n",
    "                   'cs', 'cy', 'da', 'de', 'dv', 'el', 'en', 'es', 'et', 'eu', 'fa', 'fi', 'fr', \n",
    "                   'gu', 'he', 'hi', 'hi-Latn', 'hr', 'ht', 'hu', 'hy', 'id', 'is', 'it', 'ja', \n",
    "                   'ka', 'km', 'kn', 'ko', 'lo', 'lt', 'lv', 'ml', 'mr', 'ms', 'my', 'ne', \n",
    "                   'nl', 'no', 'pa', 'pl', 'ps', 'pt', 'ro', 'ru', 'sd', 'si', 'sk', 'sl', \n",
    "                   'sr', 'sv', 'ta', 'te', 'th', 'tl', 'tr', 'ug', 'uk', 'ur', 'vi', 'zh-CN', 'zh-TW']\n",
    "'''\n",
    "category_labels = []\n",
    "#constants related to n-grams\n",
    "min_ngram_value = 2\n",
    "max_ngram_value = 6\n",
    "k = 5                  #store only top k most frequent n grams for future\n",
    "\n",
    "#constants for testing\n",
    "n_folds = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Load the processed data\n",
    "Creates the DataFrame along with the Category labels\n",
    "'''\n",
    "#create category labels from the category_fname file. \n",
    "#Returns the mapping between category-to-id in processed_tweets.json\n",
    "def create_category_labels(category_fname, data_dir_path):\n",
    "    extra_cat_added = []\n",
    "    category_fname_path = os.path.abspath(os.path.join(data_dir_path, category_fname))\n",
    "    #create the id-to-category map\n",
    "    id_category_map={}\n",
    "    with open(category_fname_path,'rb') as tsvfile:\n",
    "        tsvin = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in tsvin:\n",
    "            if row[0] not in category_labels:\n",
    "                category_labels.append(row[0])\n",
    "                extra_cat_added.append(row[0])\n",
    "            id_category_map[row[1]] = row[0]\n",
    "    \n",
    "    #print category statistics\n",
    "    #print(\"These additional categories were found in the dataset:\")\n",
    "    #print(extra_cat_added)  \n",
    "    print(\"Total number of categories now is:\",len(category_labels))\n",
    "    return id_category_map \n",
    "\n",
    "#returns a dict to store the corpus for every language <lang_code, entire_text_corpus>\n",
    "#used for BASELINE only\n",
    "def create_corpus_for_languages(data_frame):\n",
    "    language_corpus_map={}\n",
    "    for index, row in data_frame.iterrows():\n",
    "        if row[\"class\"] in language_corpus_map:\n",
    "            language_corpus_map[row[\"class\"]] = language_corpus_map[row[\"class\"]] + \" \" +row[\"content\"]\n",
    "        else:\n",
    "             language_corpus_map[row[\"class\"]] = row[\"content\"]\n",
    "    return language_corpus_map\n",
    "        \n",
    "    \n",
    "def build_data_frame(input_fname, data_dir_path, id_category_map):\n",
    "    input_fname_path = os.path.abspath(os.path.join(data_dir_path, input_fname))\n",
    "    rows = []\n",
    "    index = []\n",
    "    with open(input_fname_path,'rb') as data_file:\n",
    "        data = json.load(data_file)\n",
    "        i=0\n",
    "        for item in data:\n",
    "            rows.append({'content': item[\"content\"], 'class': id_category_map[item[\"id\"]]})\n",
    "            index.append(i)\n",
    "            i=i+1\n",
    "\n",
    "    data_frame = pd.DataFrame(rows,index=index)\n",
    "    return data_frame\n",
    "\n",
    "def load_data():\n",
    "    id_category_map = create_category_labels(category_fname, data_dir_path)\n",
    "    data_frame = build_data_frame(input_fname, data_dir_path, id_category_map)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: Extract features\n",
    "'''\n",
    "def extract_features(feature_extractor, data):\n",
    "    counts = feature_extractor.fit_transform(data)\n",
    "    return counts\n",
    "\n",
    "#extract character n-grams from the data\n",
    "def get_ngram_character_feature_extractor():\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(min_ngram_value,max_ngram_value),analyzer='char')\n",
    "    return count_vectorizer\n",
    "\n",
    "def get_ngram_word_feature_extractor(min_ngram=min_ngram_value, max_ngram=max_ngram_value, \n",
    "                                     max_features=None, vocabulary=None):\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(min_ngram,max_ngram),analyzer='word', \n",
    "                                       max_features=max_features, vocabulary=vocabulary)\n",
    "    return count_vectorizer\n",
    "\n",
    "#TODO: extract top-k character n-grams from data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: Baseline\n",
    "'''\n",
    "#BASELINE 1: classify the language if it contains the most frequent word for a language\n",
    "#If it contains words from multiple languages, \n",
    "#then use a tie breaking mechanism to classify it into that language for which \n",
    "#it has the highest frequency of the most freq word\n",
    "def baseline_1(data_frame, language_corpus_map):\n",
    "    feature_map = [get_most_freq_words(language_corpus_map[key], 1) for key in language_corpus_map]\n",
    "    \n",
    "    #flatten the feature_map\n",
    "    feature_map = list(itertools.chain(*feature_map))\n",
    "    print(type(feature_map))\n",
    "    print(feature_map)\n",
    "    \n",
    "    #fit documents into the new feature map\n",
    "    count_vectorizer = get_ngram_word_feature_extractor(1, 1, None, feature_map)\n",
    "    x = count_vectorizer.fit_transform(data_frame[\"content\"].values)\n",
    "    y = data_frame[\"class\"].values\n",
    "    predicted_y = [category_labels[predict_class(row)] for row in x]\n",
    "    \n",
    "    #calculate the accuracies\n",
    "    acc_score = accuracy_score(y, predicted_y)\n",
    "    score = f1_score(y, predicted_y, labels=category_labels, average='micro')\n",
    "    print('Total tweets classified:', len(data_frame))\n",
    "    print('Accuracy Score:', acc_score)\n",
    "    print('F1 Score:', score)\n",
    "\n",
    "#returns the top k words from the corpus\n",
    "def get_most_freq_words(corpus, k):\n",
    "    count_vectorizer = get_ngram_word_feature_extractor(1,1)\n",
    "    analyzer = count_vectorizer.build_analyzer()\n",
    "    listNgramQuery = analyzer(corpus)\n",
    "    fdist = FreqDist(listNgramQuery)\n",
    "    top_k = fdist.most_common(k)\n",
    "    features = [x[0] for x in top_k]\n",
    "    return features\n",
    "\n",
    "#get the k-th most common word\n",
    "#used when 2 different languages have the same most fequent word\n",
    "\n",
    "#returns the class of \n",
    "def predict_class(x):\n",
    "    return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 4: Classifier\n",
    "Set up the classification algorithm \n",
    "'''\n",
    "def get_bayes_classifier():\n",
    "    return MultinomialNB()\n",
    "\n",
    "def get_logistic_regression_classifier():\n",
    "    return LogisticRegression()\n",
    "\n",
    "def get_baseline_classifier(data):\n",
    "    return null\n",
    "    #for instance in data:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 5: Training\n",
    "Run the classification algorithm  \n",
    "'''\n",
    "def train_data(pipeline, data):\n",
    "\n",
    "    #set x and y \n",
    "    x = data['content'].values\n",
    "    y = data['class'].values\n",
    "\n",
    "    pipeline.fit(x, y)\n",
    "    return pipeline\n",
    "\n",
    "print(\"using NBC classifier\")\n",
    "data = load_data()\n",
    "test_x = ['hey test document']\n",
    "#set up the pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "        ('classifier',  get_bayes_classifier()) ])\n",
    "pipeline = train_data(pipeline, data)\n",
    "test_y = pipeline.predict(test_x)\n",
    "print(\"the predicted class is:\",test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 6: Testing\n",
    "'''\n",
    "#old school testing\n",
    "def test_data(pipeline, data):\n",
    "    x = data['content'].values\n",
    "    y = data['class'].values\n",
    "    train_x, test_x, train_y, test_x = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predictions = pipeline.predict(test_x)\n",
    "    acc_score = accuracy_score(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, labels=category_labels, average='micro')\n",
    "    print('Total tweets classified:', len(data))\n",
    "    print('Accuracy Score:', acc_score)\n",
    "    print('F1 Score:', score)\n",
    "    \n",
    "#k-fold testing\n",
    "def do_k_fold_testing(pipeline, data):\n",
    "    k_fold = KFold(n=len(data), n_folds=n_folds)\n",
    "    scores = []\n",
    "    f1_scores = []\n",
    "    for train_indices, test_indices in k_fold:\n",
    "        train_x = data.iloc[train_indices]['content'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values\n",
    "\n",
    "        test_x = data.iloc[test_indices]['content'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values\n",
    "\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        predictions = pipeline.predict(test_x)\n",
    "\n",
    "        acc_score = accuracy_score(test_y, predictions)\n",
    "        scores.append(acc_score)\n",
    "\n",
    "        score = f1_score(test_y, predictions, labels=category_labels, average='micro')\n",
    "        f1_scores.append(score)\n",
    "\n",
    "    #print statistics\n",
    "    print('Total tweets classified:', len(data))\n",
    "    print('Accuracy Score:', sum(scores)/len(scores))\n",
    "    print('F1 Score:', sum(f1_scores)/len(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Put it all together!\n",
    "'''\n",
    "# for bayes classifier\n",
    "data = load_data()\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "    ('classifier',  get_bayes_classifier()) ])\n",
    "do_k_fold_testing(pipeline, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for logistic regression\n",
    "data = load_data()\n",
    "pipeline2 = Pipeline([\n",
    "    ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "    ('classifier',  get_logistic_regression_classifier()) ])\n",
    "test_data(pipeline2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frame = load_data()\n",
    "language_corpus_map = create_corpus_for_languages(data_frame)\n",
    "baseline_1(data_frame, language_corpus_map)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
