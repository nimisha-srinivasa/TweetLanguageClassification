{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, os, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, itertools\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#imports for plotting confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "'''\n",
    "all constants\n",
    "'''\n",
    "\n",
    "data_dir_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'all_data'))\n",
    "input_fname = \"processed_data.json\"\n",
    "category_fname = \"uniformly_sampled.tsv\"\n",
    "confusion_matrix_fname = 'confusion_matrix.png'\n",
    "#category labels according to the website\n",
    "'''\n",
    "category_labels = ['am', 'ar', 'bg', 'bn', 'bo', 'bs', 'ca', 'ckb', \n",
    "                   'cs', 'cy', 'da', 'de', 'dv', 'el', 'en', 'es', 'et', 'eu', 'fa', 'fi', 'fr', \n",
    "                   'gu', 'he', 'hi', 'hi-Latn', 'hr', 'ht', 'hu', 'hy', 'id', 'is', 'it', 'ja', \n",
    "                   'ka', 'km', 'kn', 'ko', 'lo', 'lt', 'lv', 'ml', 'mr', 'ms', 'my', 'ne', \n",
    "                   'nl', 'no', 'pa', 'pl', 'ps', 'pt', 'ro', 'ru', 'sd', 'si', 'sk', 'sl', \n",
    "                   'sr', 'sv', 'ta', 'te', 'th', 'tl', 'tr', 'ug', 'uk', 'ur', 'vi', 'zh-CN', 'zh-TW']\n",
    "'''\n",
    "category_labels = []\n",
    "\n",
    "#constants related to n-grams     BEST RESULST: 3-4\n",
    "min_ngram_value = 3\n",
    "max_ngram_value = 4\n",
    "k = 1000                  #store only top k most frequent n grams \n",
    "\n",
    "#constants for testing\n",
    "n_folds = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Load the processed data\n",
    "Creates the DataFrame along with the Category labels\n",
    "'''\n",
    "def build_data_frame(input_fname, data_dir_path):\n",
    "    input_fname_path = os.path.abspath(os.path.join(data_dir_path, input_fname))\n",
    "    rows = []\n",
    "    index = []\n",
    "    with open(input_fname_path,'rb') as data_file:\n",
    "        data = json.load(data_file)\n",
    "        for item in data:\n",
    "            #compute the unique labels for the dataset\n",
    "            if item[\"label\"] not in category_labels:\n",
    "                category_labels.append(item[\"label\"])\n",
    "            #append the entire tuple to the row\n",
    "            rows.append({'content': item[\"content\"], 'label': item[\"label\"]})\n",
    "            index.append(item[\"id\"])\n",
    "\n",
    "    data_frame = pd.DataFrame(rows,index=index)\n",
    "    return data_frame\n",
    "\n",
    "def load_data():\n",
    "    data_frame = build_data_frame(input_fname, data_dir_path)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: Extract features\n",
    "'''\n",
    "def extract_features(feature_extractor, data):\n",
    "    counts = feature_extractor.fit_transform(data)\n",
    "    return counts\n",
    "\n",
    "#extract character n-grams from the data\n",
    "def get_ngram_character_feature_extractor(min_ngram=min_ngram_value, max_ngram=max_ngram_value):\n",
    "    print(\"min:\",min_ngram,\",max:\",max_ngram)\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(min_ngram, max_ngram),analyzer='char')\n",
    "    return count_vectorizer\n",
    "\n",
    "def get_ngram_word_feature_extractor(min_ngram=min_ngram_value, max_ngram=max_ngram_value, \n",
    "                                     max_features=None, vocabulary=None):\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(min_ngram, max_ngram),analyzer='word', \n",
    "                                       max_features=max_features, vocabulary=vocabulary)\n",
    "    return count_vectorizer\n",
    "\n",
    "#TODO: extract top-k character n-grams from data\n",
    "def get_topk_ngram_char_feature_extractor(min_ngram=min_ngram_value, max_ngram=max_ngram_value, \n",
    "                                     k=None, vocabulary=None):\n",
    "    print(min_ngram,\",\",max_ngram)\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(min_ngram, max_ngram),analyzer='char', \n",
    "                                       max_features=k, vocabulary=vocabulary)\n",
    "    return count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 4: Classifier\n",
    "Set up the classification algorithm \n",
    "'''\n",
    "def get_bayes_classifier():\n",
    "    return MultinomialNB()\n",
    "\n",
    "def get_logistic_regression_classifier():\n",
    "    return LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 5: Training\n",
    "Run the classification algorithm  \n",
    "'''\n",
    "def train_data(pipeline, data):\n",
    "\n",
    "    #set x and y \n",
    "    x = data['content'].values\n",
    "    y = data['label'].values\n",
    "\n",
    "    pipeline.fit(x, y)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 6: Testing\n",
    "'''\n",
    "#old school testing\n",
    "def test_data(pipeline, data):\n",
    "    x = data['content'].values\n",
    "    y = data['label'].values\n",
    "    train_x, test_x, train_y, test_x = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predictions = pipeline.predict(test_x)\n",
    "    acc_score = accuracy_score(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, labels=category_labels, average='micro')\n",
    "    print('Total tweets classified:', len(data))\n",
    "    print('Accuracy Score:', acc_score)\n",
    "    print('F1 Score:', score)\n",
    "    \n",
    "#k-fold testing\n",
    "def do_k_fold_testing(pipeline, data, n_folds=n_folds, plot_confusion=False):\n",
    "    k_fold = KFold(n=len(data), n_folds=n_folds)\n",
    "    scores = []\n",
    "    f1_scores = []\n",
    "    if plot_confusion:\n",
    "        confusion = np.zeros((len(category_labels),len(category_labels)))\n",
    "    for train_indices, test_indices in k_fold:\n",
    "        train_x = data.iloc[train_indices]['content'].values\n",
    "        train_y = data.iloc[train_indices]['label'].values\n",
    "\n",
    "        test_x = data.iloc[test_indices]['content'].values\n",
    "        test_y = data.iloc[test_indices]['label'].values\n",
    "\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        predictions = pipeline.predict(test_x)\n",
    "\n",
    "        acc_score = accuracy_score(test_y, predictions)\n",
    "        scores.append(acc_score)\n",
    "\n",
    "        score = f1_score(test_y, predictions, labels=category_labels, average='micro')\n",
    "        f1_scores.append(score)\n",
    "        if plot_confusion:\n",
    "            confusion += confusion_matrix(test_y, predictions,labels=category_labels)\n",
    "\n",
    "    #print statistics\n",
    "    print('Total tweets classified:', len(data))\n",
    "    print('Accuracy Score:', sum(scores)/len(scores))\n",
    "    print('F1 Score:', sum(f1_scores)/len(f1_scores))\n",
    "    \n",
    "    if plot_confusion:\n",
    "        plot_confusion_matrix(confusion, classes=category_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper section\n",
    "'''\n",
    "#Plotting the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    df_cm = pd.DataFrame(cm, index = classes,\n",
    "                  columns = classes)\n",
    "    cm_size = int(len(classes)/2)\n",
    "    plt.figure(figsize = (cm_size,cm_size))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(confusion_matrix_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Put it all together!\n",
    "'''\n",
    "# for bayes classifier\n",
    "def test_bayes_classifier():\n",
    "    data = load_data()\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "        ('classifier',  get_bayes_classifier()) ])\n",
    "    do_k_fold_testing(pipeline, data)\n",
    "\n",
    "#for logistic regression\n",
    "def test_lr_classifier():\n",
    "    data = load_data()\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "        ('classifier',  get_logistic_regression_classifier()) ])\n",
    "    test_data(pipeline2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_bayes_classifier()\n",
    "test_lr_classifier()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
