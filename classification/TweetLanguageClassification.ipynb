{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, os, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "'''\n",
    "all constants\n",
    "'''\n",
    "\n",
    "data_dir_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'all_data'))\n",
    "input_fname = \"processed_data.json\"\n",
    "category_fname = \"uniformly_sampled.tsv\"\n",
    "category_labels = ['am', 'ar', 'bg', 'bn', 'bo', 'bs', 'ca', 'ckb', \n",
    "                   'cs', 'cy', 'da', 'de', 'dv', 'el', 'en', 'es', 'et', 'eu', 'fa', 'fi', 'fr', \n",
    "                   'gu', 'he', 'hi', 'hi-Latn', 'hr', 'ht', 'hu', 'hy', 'id', 'is', 'it', 'ja', \n",
    "                   'ka', 'km', 'kn', 'ko', 'lo', 'lt', 'lv', 'ml', 'mr', 'ms', 'my', 'ne', 'nl', 'no', \n",
    "                   'pa', 'pl', 'ps', 'pt', 'ro', 'ru', 'sd', 'si', 'sk', 'sl', 'sr', 'sv', 'ta', 'te', 'th', \n",
    "                   'tl', 'tr', 'ug', 'uk', 'ur', 'vi', 'zh-CN', 'zh-TW']\n",
    "\n",
    "#constants related to n-grams\n",
    "min_ngram_value = 2\n",
    "max_ngram_value = 6\n",
    "k = 5                  #store only top k most frequent n grams for future\n",
    "\n",
    "#constants for testing\n",
    "n_folds = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Load the processed data\n",
    "Creates the DataFrame along with the Category labels\n",
    "'''\n",
    "#create category labels from the category_fname file. \n",
    "#Returns the mapping between category-to-id in processed_tweets.json\n",
    "def create_category_labels(category_fname, data_dir_path):\n",
    "    extra_cat_added = []\n",
    "    category_fname_path = os.path.abspath(os.path.join(data_dir_path, category_fname))\n",
    "    #create the category -to-id mapping\n",
    "    category_id_map={}\n",
    "    with open(category_fname_path,'rb') as tsvfile:\n",
    "        tsvin = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in tsvin:\n",
    "            if row[0] not in category_labels:\n",
    "                category_labels.append(row[0])\n",
    "                extra_cat_added.append(row[0])\n",
    "            #category_id_map[row[1]] = category_labels.index(row[0])\n",
    "            category_id_map[row[1]] = row[0]\n",
    "    \n",
    "    #print category statistics\n",
    "    print(\"These additional categories were found in the dataset:\")\n",
    "    print(extra_cat_added)  \n",
    "    print(\"Total number of categories now is:\",len(category_labels))\n",
    "    \n",
    "    return category_id_map \n",
    "    \n",
    "    \n",
    "def build_data_frame(input_fname, data_dir_path, category_id_map):\n",
    "    input_fname_path = os.path.abspath(os.path.join(data_dir_path, input_fname))\n",
    "    rows = []\n",
    "    index = []\n",
    "    with open(input_fname_path,'rb') as data_file:\n",
    "        data = json.load(data_file)\n",
    "        i=0\n",
    "        for item in data:\n",
    "            rows.append({'content': item[\"content\"], 'class': category_id_map[item[\"id\"]]})\n",
    "            index.append(i)\n",
    "            i=i+1\n",
    "\n",
    "    data_frame = pd.DataFrame(rows,index=index)\n",
    "    return data_frame\n",
    "\n",
    "def load_data():\n",
    "    category_id_map = create_category_labels(category_fname, data_dir_path)\n",
    "    data_frame = build_data_frame(input_fname, data_dir_path, category_id_map)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: Extract features\n",
    "'''\n",
    "def extract_features(feature_extractor, data):\n",
    "    counts = feature_extractor.fit_transform(data)\n",
    "    return counts\n",
    "\n",
    "#extract character n-grams from the data\n",
    "def get_ngram_character_feature_extractor():\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(min_ngram_value,max_ngram_value),analyzer='char')\n",
    "    return count_vectorizer\n",
    "\n",
    "def get_ngram_word_feature_extractor():\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(min_ngram_value,max_ngram_value),analyzer='word')\n",
    "    return count_vectorizer\n",
    "\n",
    "#TODO: extract top-k character n-grams from data\n",
    "\n",
    "#TODO: extract most frequent words from each language -- BASELINE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: Classifier\n",
    "Set up the classification algorithm \n",
    "'''\n",
    "def get_bayes_classifier():\n",
    "    return MultinomialNB()\n",
    "\n",
    "def get_logistic_regression_classifier():\n",
    "    return LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using NBC classifier\n",
      "These additional categories were found in the dataset:\n",
      "['ar_LATN', 'az', 'dv_LATN', 'gl', 'ha', 'ja_LATN', 'jv', 'ko_LATN', 'la', 'mk', 'ml_LATN', 'mn', 'mn_LATN', 'ps_LATN', 'sq', 'su', 'sw', 'ta_LATN', 'tn', 'und', 'ur_LATN', 'wo', 'xh', 'yo', 'zu']\n",
      "('Total number of categories now is:', 95)\n",
      "('the predicted class is:', 'en')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"using Logistic regression classifier\")\\n#set up the pipeline\\npipeline2 = Pipeline([\\n        (\\'vectorizer\\',  get_ngram_character_feature_extractor()),\\n        (\\'classifier\\',  get_logistic_regression_classifier()) ])\\npipeline2 = train_data(pipeline2, data)\\ntest_y = pipeline2.predict(test_x)\\nprint(\"the predicted class is:\",test_y[0])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 4: Training\n",
    "Run the classification algorithm  \n",
    "'''\n",
    "def train_data(pipeline, data):\n",
    "\n",
    "    #set x and y \n",
    "    x = data['content'].values\n",
    "    y = data['class'].values\n",
    "\n",
    "    pipeline.fit(x, y)\n",
    "    return pipeline\n",
    "\n",
    "print(\"using NBC classifier\")\n",
    "data = load_data()\n",
    "test_x = ['hey test document']\n",
    "#set up the pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "        ('classifier',  get_bayes_classifier()) ])\n",
    "pipeline = train_data(pipeline, data)\n",
    "test_y = pipeline.predict(test_x)\n",
    "print(\"the predicted class is:\",test_y[0])\n",
    "\n",
    "'''\n",
    "print(\"using Logistic regression classifier\")\n",
    "#set up the pipeline\n",
    "pipeline2 = Pipeline([\n",
    "        ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "        ('classifier',  get_logistic_regression_classifier()) ])\n",
    "pipeline2 = train_data(pipeline2, data)\n",
    "test_y = pipeline2.predict(test_x)\n",
    "print(\"the predicted class is:\",test_y[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: Testing\n",
    "'''\n",
    "#old school testing\n",
    "def test_data(pipeline, data):\n",
    "    x = data['content'].values\n",
    "    y = data['class'].values\n",
    "    train_x, test_x, train_y, test_x = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predictions = pipeline.predict(test_x)\n",
    "    acc_score = accuracy_score(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, labels=category_labels, average='micro')\n",
    "    print('Total tweets classified:', len(data))\n",
    "    print('Accuracy Score:', acc_score)\n",
    "    print('F1 Score:', score)\n",
    "    \n",
    "#k-fold testing\n",
    "def do_k_fold_testing(pipeline, data):\n",
    "    k_fold = KFold(n=len(data), n_folds=n_folds)\n",
    "    scores = []\n",
    "    f1_scores = []\n",
    "    for train_indices, test_indices in k_fold:\n",
    "        train_x = data.iloc[train_indices]['content'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values\n",
    "\n",
    "        test_x = data.iloc[test_indices]['content'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values\n",
    "\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        predictions = pipeline.predict(test_x)\n",
    "\n",
    "        acc_score = accuracy_score(test_y, predictions)\n",
    "        scores.append(acc_score)\n",
    "\n",
    "        score = f1_score(test_y, predictions, labels=category_labels, average='micro')\n",
    "        f1_scores.append(score)\n",
    "\n",
    "    #print statistics\n",
    "    print('Total tweets classified:', len(data))\n",
    "    print('Accuracy Score:', sum(scores)/len(scores))\n",
    "    print('F1 Score:', sum(f1_scores)/len(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These additional categories were found in the dataset:\n",
      "[]\n",
      "('Total number of categories now is:', 95)\n",
      "('Total tweets classified:', 71444)\n",
      "('Accuracy Score:', 0.62946597279214356)\n",
      "('F1 Score:', 0.62946597279214356)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Put it all together!\n",
    "'''\n",
    "# for bayes classifier\n",
    "data = load_data()\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "    ('classifier',  get_bayes_classifier()) ])\n",
    "do_k_fold_testing(pipeline, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for logistic regression\n",
    "data = load_data()\n",
    "pipeline2 = Pipeline([\n",
    "    ('vectorizer',  get_ngram_character_feature_extractor()),\n",
    "    ('classifier',  get_logistic_regression_classifier()) ])\n",
    "test_data(pipeline2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "'''\n",
    "Step 3: Get the most frequent top k ngrams only!\n",
    "'''\n",
    "#get frequency distrubution of ngrams in the corpus\n",
    "'''\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "listNgramQuery = analyzer(corpus)\n",
    "NgramQueryWeights = nltk.FreqDist(listNgramQuery)\n",
    "print(NgramQueryWeights.most_common(k))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
