{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "'''\n",
    "all constants\n",
    "'''\n",
    "\n",
    "data_dir_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'all_data'))\n",
    "input_fname = \"processed_data.json\"\n",
    "k=10\n",
    "category_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Load the processed data\n",
    "Creates the DataFrame along with the Category labels\n",
    "'''\n",
    "def build_data_frame(input_fname, data_dir_path):\n",
    "    input_fname_path = os.path.abspath(os.path.join(data_dir_path, input_fname))\n",
    "    rows = []\n",
    "    index = []\n",
    "    with open(input_fname_path,'rb') as data_file:\n",
    "        data = json.load(data_file)\n",
    "        for item in data:\n",
    "            #compute the unique labels for the dataset\n",
    "            if item[\"label\"] not in category_labels:\n",
    "                category_labels.append(item[\"label\"])\n",
    "            #append the entire tuple to the row\n",
    "            rows.append({'content': item[\"content\"], 'label': item[\"label\"]})\n",
    "            index.append(item[\"id\"])\n",
    "\n",
    "    data_frame = pd.DataFrame(rows,index=index)\n",
    "    return data_frame\n",
    "\n",
    "def load_data():\n",
    "    data_frame = build_data_frame(input_fname, data_dir_path)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: Extract features for training\n",
    "'''\n",
    "def get_ngram_word_feature_extractor(min_ngram=1, max_ngram=1, \n",
    "                                     max_features=None, \n",
    "                                     vocabulary=None, \n",
    "                                     binary=True):\n",
    "    count_vectorizer = CountVectorizer(ngram_range = (min_ngram, max_ngram), analyzer = 'word', \n",
    "                                       max_features = max_features, binary = binary, vocabulary = vocabulary)\n",
    "    return count_vectorizer\n",
    "\n",
    "#returns a dict to store the corpus for every language <lang_code, [tweet1, tweet2,.. tweet_n]>\n",
    "def create_corpus_for_languages(data_frame):\n",
    "    language_corpus_map={}\n",
    "    for index, row in data_frame.iterrows():\n",
    "        if row[\"label\"] not in language_corpus_map:\n",
    "            language_corpus_map[row[\"label\"]] = []\n",
    "        language_corpus_map[row[\"label\"]].append(row[\"content\"]) \n",
    "    return language_corpus_map\n",
    "\n",
    "#returns the top k most frequent data for langauge_data where the text is stored as a list\n",
    "def find_topK_words_per_langauge(language_data, k=k):\n",
    "    vec = get_ngram_word_feature_extractor()\n",
    "    X = vec.fit_transform(language_data)\n",
    "    counts = zip(vec.get_feature_names(),np.asarray(X.sum(axis=0)).ravel())\n",
    "    most_frequent = sorted(counts, key=lambda tup: tup[1], reverse=True)[:k]\n",
    "    return most_frequent\n",
    "\n",
    "def train_data(data_frame):\n",
    "    language_corpus_map = create_corpus_for_languages(data_frame)\n",
    "    lang_mostFreqWord_map = {}\n",
    "    for language in language_corpus_map:\n",
    "        lang_mostFreqWord_map[language] = find_topK_words_per_langauge(language_corpus_map[language])\n",
    "    return lang_mostFreqWord_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: Test the data\n",
    "'''\n",
    "#returns the predicted language of the tweet\n",
    "def test_single_tweet(tweet, lang_mostFreqWord_map):\n",
    "    string_label = []\n",
    "    for word in tweet.split():\n",
    "        for symbol in lang_mostFreqWord_map:\n",
    "            for i in lang_mostFreqWord_map[symbol]:\n",
    "                if i[0] == word:\n",
    "                    string_label.append(symbol)\n",
    "    if len(string_label) == 0:\n",
    "        string_label.append('en')\n",
    "    c = Counter(string_label)\n",
    "    v=list(c.values())\n",
    "    k=list(c.keys())\n",
    "    return k[v.index(max(v))]\n",
    "\n",
    "def test_data(data, lang_mostFreqWord_map):\n",
    "    predicted = []\n",
    "    for index, row in data.iterrows():\n",
    "        predicted_class = test_single_tweet(row['content'], lang_mostFreqWord_map)\n",
    "        predicted.append(predicted_class)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(true_y, predicted_y):\n",
    "    print(accuracy_score(true_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.539420249265\n"
     ]
    }
   ],
   "source": [
    "def do_baseline():\n",
    "    data_frame = load_data()\n",
    "    \n",
    "    #create train & test data from the original data\n",
    "    train, test = train_test_split(data_frame, test_size = 0.2)\n",
    "    lang_mostFreqWord_map = train_data(train)\n",
    "    predicted = test_data(test, lang_mostFreqWord_map)\n",
    "    calculate_metrics(test['label'].values, predicted)\n",
    "\n",
    "do_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
