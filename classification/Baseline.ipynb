{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create category labels from the category_fname file. \n",
    "#Returns the mapping between category-to-id in processed_tweets.json\n",
    "#used for BASELINE only\n",
    "def create_category_labels(category_fname, data_dir_path):\n",
    "    extra_cat_added = []\n",
    "    category_fname_path = os.path.abspath(os.path.join(data_dir_path, category_fname))\n",
    "    #create the id-to-category map\n",
    "    id_category_map={}\n",
    "    with open(category_fname_path,'rb') as tsvfile:\n",
    "        tsvin = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in tsvin:\n",
    "            if row[0] not in category_labels:\n",
    "                category_labels.append(row[0])\n",
    "                extra_cat_added.append(row[0])\n",
    "            id_category_map[row[1]] = row[0]\n",
    "    \n",
    "    #print category statistics\n",
    "    #print(\"These additional categories were found in the dataset:\")\n",
    "    #print(extra_cat_added)  \n",
    "    print(\"Total number of categories now is:\",len(category_labels))\n",
    "    return id_category_map \n",
    "\n",
    "#returns a dict to store the corpus for every language <lang_code, entire_text_corpus>\n",
    "#used for BASELINE only\n",
    "def create_corpus_for_languages(data_frame):\n",
    "    language_corpus_map={}\n",
    "    for index, row in data_frame.iterrows():\n",
    "        if row[\"label\"] in language_corpus_map:\n",
    "            language_corpus_map[row[\"label\"]] = language_corpus_map[row[\"label\"]] + \" \" +row[\"content\"]\n",
    "        else:\n",
    "             language_corpus_map[row[\"label\"]] = row[\"content\"]\n",
    "    return language_corpus_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: Baseline\n",
    "'''\n",
    "#BASELINE 1: classify the language if it contains the most frequent word for a language\n",
    "#If it contains words from multiple languages, \n",
    "#then use a tie breaking mechanism to classify it into that language for which \n",
    "#it has the highest frequency of the most freq word\n",
    "def baseline_1(data_frame, language_corpus_map, category_labels):\n",
    "    feature_map = []\n",
    "    for key in language_corpus_map:\n",
    "        new_feature = get_topk_freq_words(language_corpus_map[key],1)\n",
    "        k=2\n",
    "        while new_feature in feature_map:\n",
    "            prev_lang_index = feature_map.index(new_feature)\n",
    "            print(new_feature,\" already in feature map for k= \",k, \" key=\",key,\" old_lang=\",category_labels[prev_lang_index])\n",
    "            features_prev_lang = get_topk_freq_words(language_corpus_map[category_labels[prev_lang_index]], k)\n",
    "            new_feature_prev_lang = features_prev_lang[k-1]  if len(features_prev_lang)>=k else features_prev_lang[len(features_prev_lang)-1]\n",
    "            feature_map[prev_lang_index] = new_feature_prev_lang\n",
    "            \n",
    "            features_new_lang = get_topk_freq_words(language_corpus_map[key], k)\n",
    "            new_feature = features_prev_lang[k-1]  if len(features_prev_lang)>=k else features_prev_lang[len(features_prev_lang)-1]\n",
    "\n",
    "            k = k+1\n",
    "        feature_map.append(new_feature)\n",
    "    \n",
    "    #flatten the feature_map\n",
    "    feature_map = list(itertools.chain(*feature_map))\n",
    "    print(type(feature_map))\n",
    "    print(feature_map)\n",
    "    \n",
    "    #fit documents into the new feature map\n",
    "    count_vectorizer = get_ngram_word_feature_extractor(1, 1, None, feature_map)\n",
    "    x = count_vectorizer.fit_transform(data_frame[\"content\"].values)\n",
    "    y = data_frame[\"label\"].values\n",
    "    predicted_y = [category_labels[predict_class(row)] for row in x]\n",
    "    \n",
    "    #calculate the accuracies\n",
    "    acc_score = accuracy_score(y, predicted_y)\n",
    "    score = f1_score(y, predicted_y, labels=category_labels, average='micro')\n",
    "    print('Total tweets classified:', len(data_frame))\n",
    "    print('Accuracy Score:', acc_score)\n",
    "    print('F1 Score:', score)\n",
    "\n",
    "#returns the top k words from the corpus\n",
    "def get_topk_freq_words(corpus, k):\n",
    "    count_vectorizer = get_ngram_word_feature_extractor(1,1)\n",
    "    analyzer = count_vectorizer.build_analyzer()\n",
    "    listNgramQuery = analyzer(corpus)\n",
    "    fdist = FreqDist(listNgramQuery)\n",
    "    top_k = fdist.most_common(k)\n",
    "    features = [x[0] for x in top_k]\n",
    "    return features\n",
    "\n",
    "#get the k-th most common word\n",
    "#used when 2 different languages have the same most fequent word\n",
    "def get_kth_freq_word(corpus, k):\n",
    "    count_vectorizer = get_ngram_word_feature_extractor(1,1)\n",
    "    analyzer = count_vectorizer.build_analyzer()\n",
    "    listNgramQuery = analyzer(corpus)\n",
    "    fdist = FreqDist(listNgramQuery)\n",
    "    top_k = fdist.most_common(k)\n",
    "    print(\"in kth freq word\")\n",
    "    return top_k[k-1][0] if len(top_k)==k else top_k[len(top_k)-1]\n",
    "\n",
    "#returns the class of \n",
    "def predict_class(x):\n",
    "    return np.argmax(x,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
